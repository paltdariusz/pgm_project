{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "from random import randrange\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Formatted Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Precip Type</th>\n",
       "      <th>Temperature (C)</th>\n",
       "      <th>Apparent Temperature (C)</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed (km/h)</th>\n",
       "      <th>Wind Bearing (degrees)</th>\n",
       "      <th>Visibility (km)</th>\n",
       "      <th>Pressure (millibars)</th>\n",
       "      <th>Daily Summary</th>\n",
       "      <th>Temperature (K)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-01 00:00:00+00:00</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>rain</td>\n",
       "      <td>1.161111</td>\n",
       "      <td>-3.238889</td>\n",
       "      <td>0.85</td>\n",
       "      <td>16.6152</td>\n",
       "      <td>139.0</td>\n",
       "      <td>9.9015</td>\n",
       "      <td>1016.15</td>\n",
       "      <td>Mostly cloudy throughout the day.</td>\n",
       "      <td>274.311111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-01 06:00:00+00:00</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>rain</td>\n",
       "      <td>2.072222</td>\n",
       "      <td>-3.272222</td>\n",
       "      <td>0.81</td>\n",
       "      <td>25.4219</td>\n",
       "      <td>136.0</td>\n",
       "      <td>10.0303</td>\n",
       "      <td>1013.24</td>\n",
       "      <td>Mostly cloudy throughout the day.</td>\n",
       "      <td>275.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-01 12:00:00+00:00</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>rain</td>\n",
       "      <td>6.205556</td>\n",
       "      <td>2.238889</td>\n",
       "      <td>0.76</td>\n",
       "      <td>23.4094</td>\n",
       "      <td>141.0</td>\n",
       "      <td>10.3523</td>\n",
       "      <td>1011.18</td>\n",
       "      <td>Mostly cloudy throughout the day.</td>\n",
       "      <td>279.355556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-01 18:00:00+00:00</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>rain</td>\n",
       "      <td>5.472222</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.83</td>\n",
       "      <td>25.4058</td>\n",
       "      <td>145.0</td>\n",
       "      <td>10.9158</td>\n",
       "      <td>1009.55</td>\n",
       "      <td>Mostly cloudy throughout the day.</td>\n",
       "      <td>278.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-02 00:00:00+00:00</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>rain</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>-1.550000</td>\n",
       "      <td>0.91</td>\n",
       "      <td>15.2950</td>\n",
       "      <td>221.0</td>\n",
       "      <td>5.3130</td>\n",
       "      <td>1014.72</td>\n",
       "      <td>Overcast throughout the day.</td>\n",
       "      <td>275.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Formatted Date        Summary Precip Type  Temperature (C)  \\\n",
       "0  2006-01-01 00:00:00+00:00  Mostly Cloudy        rain         1.161111   \n",
       "1  2006-01-01 06:00:00+00:00  Mostly Cloudy        rain         2.072222   \n",
       "2  2006-01-01 12:00:00+00:00  Mostly Cloudy        rain         6.205556   \n",
       "3  2006-01-01 18:00:00+00:00       Overcast        rain         5.472222   \n",
       "4  2006-01-02 00:00:00+00:00       Overcast        rain         2.350000   \n",
       "\n",
       "   Apparent Temperature (C)  Humidity  Wind Speed (km/h)  \\\n",
       "0                 -3.238889      0.85            16.6152   \n",
       "1                 -3.272222      0.81            25.4219   \n",
       "2                  2.238889      0.76            23.4094   \n",
       "3                  1.100000      0.83            25.4058   \n",
       "4                 -1.550000      0.91            15.2950   \n",
       "\n",
       "   Wind Bearing (degrees)  Visibility (km)  Pressure (millibars)  \\\n",
       "0                   139.0           9.9015               1016.15   \n",
       "1                   136.0          10.0303               1013.24   \n",
       "2                   141.0          10.3523               1011.18   \n",
       "3                   145.0          10.9158               1009.55   \n",
       "4                   221.0           5.3130               1014.72   \n",
       "\n",
       "                       Daily Summary  Temperature (K)  \n",
       "0  Mostly cloudy throughout the day.       274.311111  \n",
       "1  Mostly cloudy throughout the day.       275.222222  \n",
       "2  Mostly cloudy throughout the day.       279.355556  \n",
       "3  Mostly cloudy throughout the day.       278.622222  \n",
       "4       Overcast throughout the day.       275.500000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/filtered_weather_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_EPOCHS = 25 \n",
    "DEFAULT_BATCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear-chain Conditional Random Field (CRF).\n",
    "\n",
    "    based on: https://github.com/mtreviso/linear-chain-crf/blob/master/crf_vectorized.py\n",
    "\n",
    "    Args:\n",
    "        nb_labels (int): number of labels in your dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_labels = nb_labels\n",
    "        self.PAD_ID = -1\n",
    "\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.empty(self.nb_labels + 1, self.nb_labels + 1)\n",
    "        )\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "        self.transitions.data[self.PAD_ID, :] = -10000.0\n",
    "        self.transitions.data[:, self.PAD_ID] = -10000.0\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Select best path for given data.\"\"\"\n",
    "        return self.decode(x, mask=mask)\n",
    "\n",
    "    def log_likelihood(self, emissions, labels, mask=None):\n",
    "        \"\"\"Compute the probability of a sequence of labels given a sequence of\n",
    "        emissions scores.\n",
    "        Args:\n",
    "            emissions (torch.Tensor): Sequence of emissions for each label.\n",
    "                Shape of (batch_size, seq_len, nb_labels).\n",
    "            labels (torch.LongTensor): Sequence of labels.\n",
    "                Shape of (batch_size, seq_len).\n",
    "            mask (torch.FloatTensor, optional): Tensor representing valid positions.\n",
    "                If None, all positions are considered valid.\n",
    "                Shape of (batch_size, seq_len).\n",
    "        Returns:\n",
    "            torch.Tensor: the log-likelihoods for each sequence in the batch.\n",
    "                Shape of (batch_size,)\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.shape[:2], dtype=torch.float)\n",
    "\n",
    "        scores = self._compute_scores(emissions, labels, mask=mask)\n",
    "        partition = self._compute_log_partition(emissions, mask=mask)\n",
    "        return scores - partition\n",
    "\n",
    "    def decode(self, emissions, mask=None):\n",
    "        \"\"\"Find the most probable sequence of labels given the emissions using\n",
    "        the Viterbi algorithm.\n",
    "        Args:\n",
    "            emissions (torch.Tensor): Sequence of emissions for each label.\n",
    "                Shape (batch_size, seq_len, nb_labels).\n",
    "            mask (torch.FloatTensor, optional): Tensor representing valid positions.\n",
    "                If None, all positions are considered valid.\n",
    "                Shape (batch_size, seq_len).\n",
    "        Returns:\n",
    "            torch.Tensor: the viterbi score for the for each batch.\n",
    "                Shape of (batch_size,)\n",
    "            list of lists: the best viterbi sequence of labels for each batch.\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.shape[:2], dtype=torch.float)\n",
    "\n",
    "        scores, sequences = self._viterbi_decode(emissions, mask)\n",
    "        return scores, sequences\n",
    "\n",
    "    def _compute_scores(self, emissions, labels, mask):\n",
    "        \"\"\"Compute the scores for a given batch of emissions with their labels.\n",
    "        Args:\n",
    "            emissions (torch.Tensor): (batch_size, seq_len, nb_labels)\n",
    "            labels (Torch.LongTensor): (batch_size, seq_len)\n",
    "            mask (Torch.FloatTensor): (batch_size, seq_len)\n",
    "        Returns:\n",
    "            torch.Tensor: Scores for each batch.\n",
    "                Shape of (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = labels.shape\n",
    "        scores = torch.zeros(batch_size)\n",
    "\n",
    "        current = labels[:, 0]\n",
    "\n",
    "        # add the [unary] emission scores for the first labels for each batch\n",
    "        # for all batches, the first label, see the correspondent emissions\n",
    "        # for the first labels (which is a list of ids):\n",
    "        # emissions[:, 0, [label_1, label_2, ..., label_nblabels]]\n",
    "        scores += emissions[:, 0].gather(1, current.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # now lets do this for each remaining labels\n",
    "        for i in range(1, seq_length):\n",
    "            # we could: iterate over batches, check if we reached a mask symbol\n",
    "            # and stop the iteration, but vecotrizing is faster,\n",
    "            # so instead we perform an element-wise multiplication\n",
    "            is_valid = mask[:, i].int()\n",
    "\n",
    "            previous = current\n",
    "            current = labels[:, i]\n",
    "\n",
    "            # calculate emission and transition scores as we did before\n",
    "            e_scores = emissions[:, i].gather(1, current.unsqueeze(1)).squeeze()\n",
    "            t_scores = self.transitions[previous, current]\n",
    "\n",
    "            # apply the mask\n",
    "            e_scores = e_scores * is_valid\n",
    "            t_scores = t_scores * is_valid\n",
    "\n",
    "            scores += e_scores + t_scores\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def _compute_log_partition(self, emissions, mask):\n",
    "        \"\"\"Compute the partition function in log-space using the forward-algorithm.\n",
    "        Args:\n",
    "            emissions (torch.Tensor): (batch_size, seq_len, nb_labels)\n",
    "            mask (Torch.FloatTensor): (batch_size, seq_len)\n",
    "        Returns:\n",
    "            torch.Tensor: the partition scores for each batch.\n",
    "                Shape of (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, nb_labels = emissions.shape\n",
    "\n",
    "        alphas = emissions[:, 0]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # (bs, nb_labels) -> (bs, 1, nb_labels)\n",
    "            e_scores = emissions[:, i].unsqueeze(1)\n",
    "\n",
    "            # (nb_labels, nb_labels) -> (bs, nb_labels, nb_labels)\n",
    "            t_scores = self.transitions.unsqueeze(0)\n",
    "\n",
    "            # (bs, nb_labels)  -> (bs, nb_labels, 1)\n",
    "            a_scores = alphas.unsqueeze(2)\n",
    "\n",
    "            scores = e_scores + t_scores + a_scores\n",
    "            new_alphas = torch.logsumexp(scores, dim=1)\n",
    "\n",
    "            # set alphas if the mask is valid, otherwise keep the current values\n",
    "            is_valid = mask[:, i].unsqueeze(-1).int()\n",
    "            alphas = is_valid * new_alphas + (1 - is_valid) * alphas\n",
    "\n",
    "        # return a *log* of sums of exps\n",
    "        return torch.logsumexp(alphas, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions, mask):\n",
    "        \"\"\"Compute the viterbi algorithm to find the most probable sequence of labels\n",
    "        given a sequence of emissions.\n",
    "        Args:\n",
    "            emissions (torch.Tensor): (batch_size, seq_len, nb_labels)\n",
    "            mask (Torch.FloatTensor): (batch_size, seq_len)\n",
    "        Returns:\n",
    "            torch.Tensor: the viterbi score for the for each batch.\n",
    "                Shape of (batch_size,)\n",
    "            torch.Tensor: the best viterbi sequence of labels for each batch\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, nb_labels = emissions.shape\n",
    "\n",
    "        alphas = emissions[:, 0]\n",
    "\n",
    "        backpointers = []\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # (bs, nb_labels) -> (bs, 1, nb_labels)\n",
    "            e_scores = emissions[:, i].unsqueeze(1)\n",
    "\n",
    "            # (nb_labels, nb_labels) -> (bs, nb_labels, nb_labels)\n",
    "            t_scores = self.transitions.unsqueeze(0)\n",
    "\n",
    "            # (bs, nb_labels)  -> (bs, nb_labels, 1)\n",
    "            a_scores = alphas.unsqueeze(2)\n",
    "\n",
    "            # combine current scores with previous alphas\n",
    "            scores = e_scores + t_scores + a_scores\n",
    "\n",
    "            # so far is exactly like the forward algorithm,\n",
    "            # but now, instead of calculating the logsumexp,\n",
    "            # we will find the highest score and the label associated with it\n",
    "            max_scores, max_score_labels = torch.max(scores, dim=1)\n",
    "\n",
    "            # set alphas if the mask is valid, otherwise keep the current values\n",
    "            is_valid = mask[:, i].unsqueeze(-1)\n",
    "            alphas = is_valid * max_scores + (1 - is_valid) * alphas\n",
    "\n",
    "            # add the max_score_labels for our list of backpointers\n",
    "            # max_scores has shape (batch_size, nb_labels) so we transpose it to\n",
    "            # be compatible with our previous loopy version of viterbi\n",
    "            backpointers.append(max_score_labels.t())\n",
    "\n",
    "        # get the final most probable score and the final most probable label\n",
    "        max_final_scores, max_final_labels = torch.max(alphas, dim=1)\n",
    "\n",
    "        # find the best sequence of labels for each sample in the batch\n",
    "        best_sequences = []\n",
    "        emission_lengths = mask.int().sum(dim=1)\n",
    "        for i in range(batch_size):\n",
    "            # recover the original sentence length for the i-th sample in the batch\n",
    "            sample_length = emission_lengths[i].item()\n",
    "\n",
    "            # recover the max label for the last timestep\n",
    "            sample_final_label = max_final_labels[i].item()\n",
    "\n",
    "            # limit the backpointers until the last but one\n",
    "            # since the last corresponds to the sample_final_label\n",
    "            sample_backpointers = backpointers[: sample_length - 1]\n",
    "\n",
    "            # follow the backpointers to build the sequence of labels\n",
    "            sample_path = self._find_best_path(\n",
    "                i, sample_final_label, sample_backpointers\n",
    "            )\n",
    "\n",
    "            # add this path to the list of best sequences\n",
    "            best_sequences.append(sample_path)\n",
    "\n",
    "        return max_final_scores, torch.tensor(best_sequences)\n",
    "\n",
    "    def _find_best_path(self, sample_id, best_label, backpointers):\n",
    "        \"\"\"Auxiliary function to find the best path sequence for a specific sample.\n",
    "        Args:\n",
    "            sample_id (int): sample index in the range [0, batch_size)\n",
    "            best_label (int): label which maximizes the final score\n",
    "            backpointers (list of lists of tensors): list of pointers with\n",
    "            shape (seq_len_i-1, nb_labels, batch_size) where seq_len_i\n",
    "            represents the length of the ith sample in the batch\n",
    "        Returns:\n",
    "            list of ints: a list of label indexes representing the bast path\n",
    "        \"\"\"\n",
    "        # add the final best_label to our best path\n",
    "        best_path = [best_label]\n",
    "\n",
    "        # traverse the backpointers in backwards\n",
    "        for backpointers_t in reversed(backpointers):\n",
    "            # recover the best_label at this timestep\n",
    "            best_label = backpointers_t[best_label][sample_id].item()\n",
    "            best_path.append(best_label)\n",
    "\n",
    "        return best_path[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifier(nn.Module):\n",
    "    \"\"\"Helper class for implementing sequence classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_labels: int, x_size: int):\n",
    "        \"\"\"\n",
    "        :param nb_labels: number of labels in the target (shape of the output layer)\n",
    "        :param x_size: size of x input to determin first layer size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nb_labels = nb_labels\n",
    "        self.x_size = x_size\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Pass data through the model.\n",
    "\n",
    "        .. note: this function should return values used for calculating the loss function\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run prediction on given data.\n",
    "\n",
    "        .. note: this function should return labels for all corresponding features\n",
    "\n",
    "        :param x: input features tensor of shape (batch_size, seq_length, x_size)\n",
    "        :returns: labels tensor of shape (batch_size, seq_length,)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
